{"posts":[{"title":"SSH操作","content":"端口转发 本地转发 本地转发（local forwarding）指的是，创建一个本地端口，将发往该端口的所有通信都通过 SSH 服务器，转发到指定的远程服务器的端口。这种情况下，SSH 服务器只是一个作为跳板的中介，用于连接本地计算机无法直接连接的远程服务器。本地转发是在本地计算机建立的转发规则。 它的语法如下，其中会指定本地端口（local-port）、SSH 服务器（tunnel-host）、远程服务器（target-host）和远程端口（target-port）。 # 与 tunnel-host 建立ssh channel, 当访问本机的 local-port 使， 将请求传输到 tunnel-host 上， 由 tunnel-host 向 target-host 的 target-port 发起请求，获取数据后返回本机。 # 可以理解 tunnel-host 是个代理 ssh -L -N -f local-port:target-host:target-port tunnel-host 有三个配置参数： -L：转发本地端口。 -N：不发送任何命令，只用来建立连接。没有这个参数，会在 SSH 服务器打开一个 Shell。 -f：将 SSH 连接放到后台。没有这个参数，暂时不用 SSH 连接时，终端会失去响应。 如果经常使用本地转发，可以将设置写入 SSH 客户端的用户个人配置文件（~/.ssh/config）。 Host test.example.com LocalForward client-IP:client-port server-IP:server-port 远程转发 远程转发指的是在远程 SSH 服务器建立的转发规则。 它跟本地转发正好反过来。建立本地计算机到远程 SSH 服务器的隧道以后，本地转发是通过本地计算机访问远程 SSH 服务器，而远程转发则是通过远程 SSH 服务器访问本地计算机（访问远程SSH服务器端口， 请求会转发到本地的端口上）。 $ ssh -R remote-port:target-host:target-port -N remotehost 上面命令中，-R参数表示远程端口转发，remote-port是远程 SSH 服务器的端口，target-host和target-port是目标服务器及其端口，remotehost是远程 SSH 服务器。 如果经常执行远程端口转发，可以将设置写入 SSH 客户端的用户个人配置文件（~/.ssh/config）。 Host remote-forward HostName test.example.com RemoteForward remote-port target-host:target-port 完成上面的设置后，执行下面的命令就会建立远程转发。 $ ssh -N remote-forward # 等同于 $ ssh -R remote-port:target-host:target-port -N test.example.com 动态转发 动态转发指的是，本机与 SSH 服务器之间创建了一个加密连接，然后本机内部针对某个端口的通信，都通过这个加密连接转发。它的一个使用场景就是，访问所有外部网站，都通过 SSH 转发。 动态转发需要把本地端口绑定到 SSH 服务器。至于 SSH 服务器要去访问哪一个网站，完全是动态的，取决于原始通信，所以叫做动态转发。 $ ssh -D local-port tunnel-host -N 上面命令中，-D表示动态转发，local-port是本地端口，tunnel-host是 SSH 服务器，-N表示这个 SSH 连接只进行端口转发，不登录远程 Shell，不能执行远程命令，只能充当隧道。 举例来说，如果本地端口是2121，那么动态转发的命令就是下面这样。 $ ssh -D 2121 tunnel-host -N 注意，这种转发采用了 SOCKS5 协议。访问外部网站时，需要把 HTTP 请求转成 SOCKS5 协议，才能把本地端口的请求转发出去。 下面是 SSH 隧道建立后的一个使用实例。 $ curl -x socks5://localhost:2121 http://www.example.com 上面命令中，curl 的-x参数指定代理服务器，即通过 SOCKS5 协议的本地2121端口，访问http://www.example.com。 如果经常使用动态转发，可以将设置写入 SSH 客户端的用户个人配置文件（~/.ssh/config）。 DynamicForward tunnel-host:local-port Simple Operation - Connect to a remote server: ssh username@remote_host - Connect to a remote server with a specific identity (private key): ssh -i path/to/key_file username@remote_host - Connect to a remote server using a specific [p]ort: ssh username@remote_host -p 2222 - Run a command on a remote server with a [t]ty allocation allowing interaction with the remote command: ssh username@remote_host -t command command_arguments - SSH tunneling: [D]ynamic port forwarding (SOCKS proxy on localhost:1080): ssh -D 1080 username@remote_host - SSH tunneling: Forward a specific port (localhost:9999 to example.org:80) along with disabling pseudo-[T]ty alloc ation and executio[N] of remote commands: ssh -L 9999:example.org:80 -N -T username@remote_host - SSH [J]umping: Connect through a jumphost to a remote server (Multiple jump hops may be specified separated by co mma characters): ssh -J username@jump_host username@remote_host - Close a hanged session: &lt;Enter&gt; ~ . FAQ 提示错误内容为： no matching host key type found. Their offer: ssh-rsa 两种解决办法： 临时 ssh 增加参数 -o HostKeyAlgorithms=+ssh-rsa 编辑文件~/.ssh/config， 增加以下内容： HostKeyAlgorithms +ssh-rsa PubkeyAcceptedKeyTypes +ssh-rsa # StrictHostKeyChecking和CheckHostIP参数也可以加入这个文件 scp命令提示：/usr/libexec/sftp-server: not found，但存在sftp-server 可能是服务不兼容问题，尝试使用-O参数，让scp以传统模式(SSH Channel)来传输文件。 ","link":"https://blog.wenmingmin.site/post/ssh-cao-zuo/"},{"title":"PersonalToolsDeploy","content":"个人使用的工具部署记录 Alist Docker 参考 alist官方文档Docker部署 Systemd 按照alist官方文档下载二进制包，部署并运行。 编辑文件 /etc/systemd/system/alist.service ， 写入内容： [Unit] Description=alist server After=network-online.target Before=nss-lookup.target Wants=network-online.target nss-lookup.target [Service] WorkingDirectory=/home/arlen/tools/ ExecStart=/home/arlen/tools/alist server ExecStop=/bin/kill -SIGINT $MAINPID User=arlen [Install] WantedBy=multi-user.target Swagger Docker Compose deploy services: swagger-editor: image: swaggerapi/swagger-editor:latest ports: - 4567:8080 volumes: - /home/arlen/Documents/swagger/:/workspace/ restart: unless-stopped environment: SWAGGER_FILE: &quot;/workspace/group.json&quot; swagger-ui: image: swaggerapi/swagger-ui:v5.18.2 ports: - 8080:8080 volumes: - /home/arlen/Documents/swagger/:/usr/share/nginx/html/workspace/ restart: unless-stopped environment: SWAGGER_JSON_URL: &quot;http://127.0.0.1:8080/workspace/openapi.json&quot; Deploy Single Swagger Editor docker run -d -p 4567:8080 -v /home/arlen/Documents/swagger/:/workspace/ -e SWAGGER_FILE=/workspace/group.json --name=swagger-editor swaggerapi/swagger-editor:latest Swagger UI docker run -d -p 8080:8080 -e SWAGGER_JSON_URL=http://127.0.0.1:8080/workspace/openapi.json -v /home/arlen/Documents/swagger/:/usr/share/nginx/html/workspace/ --name=swagger-ui swaggerapi/swagger-ui:v5.18.2 命令行工具 tldr Too Long; Didn't Read; 简化了man命令输出，只列出常用的几个命令参数以及说明; github: https://github.com/tldr-pages/tldr hstr 记录终端执行过的历史命令， 并提供快速查找功能。 github: https://github.com/dvorka/hstr mdbook 渲染markdown文件为web站点，并以书籍目录的方式展示相关联的markdown文件。 使用book.toml来描述整个项目。 | book_dir | -- book.toml 项目描述文件 | -- book 生成的站点文件 | -- src markdown文件根目录 | -- | SUMMARY.md 目录文件 | -- | other markdown files # 当前目录为根目录， 渲染站点并展示，监听端口8912， 允许任意来源ip的连接 mdbook serve ./ -p 8912 -n 0.0.0.0 #!/usr/bin/python3 # -*- encoding: UTF-8 -*- # Author: arlen # filename: generate_summary.py # usage: generate SUMMARY.md by scan all md files &quot;&quot;&quot; 将该脚本放在mdbook目录结构中的src目录下， 执行python3 generate_summary.py 扫描脚本所在目录下的所有md文件以及文件夹， 文件夹必须要有一个与目录同名的md文件 例如目录 ./abcd ，需要一个文件 ./abcd.md 生成SUMMARY.md文件中内容，供mdbook程序使用 &quot;&quot;&quot; import os IGNORED_PREFIX = &quot;_&quot; ALLOWED_FILE_SUFFIX = [&quot;md&quot;] FILE_SEP = &quot;.&quot; SOURCE_DIR = &quot;src&quot; STATIC_DIR = &quot;static&quot; SUMMARY_TEMPLATE_FILENAME = &quot;SUMMARY.md&quot; SUMMARY_TEMPLATE = &quot;# Summary&quot; LINK_TEMPLATE = &quot;{prefix}- [{name}]({path})&quot; DIR_TEMPLATE = &quot;{prefix}- {name}&quot; def get_md_files(current_path=None): if current_path is None: current_path = os.getcwd() file_list = {} for i in os.listdir(current_path): if i == STATIC_DIR: continue if i.startswith(IGNORED_PREFIX): continue if i == SUMMARY_TEMPLATE_FILENAME: continue i_path = os.path.join(current_path, i) if os.path.isdir(i_path): if not os.path.isfile(&quot;{}.md&quot;.format(i_path)): raise Exception(&quot;not exist the same name file{} with dir{}&quot;.format(&quot;{}.md&quot;.format(i_path), i_path)) if i in file_list.keys(): file_list[i][&quot;child&quot;] = get_md_files(i_path) else: file_list[i] = {&quot;name&quot;: i, &quot;path&quot;: i_path, &quot;child&quot;: get_md_files(i_path)} if os.path.isfile(i_path): if not is_md_file(i_path): continue name = FILE_SEP.join(i.split(FILE_SEP)[:-1]) if name in file_list.keys(): file_list[name][&quot;path&quot;] = i_path else: file_list[name] = {&quot;name&quot;: name, &quot;path&quot;: i_path, &quot;child&quot;: []} return file_list def is_md_file(file_path): if file_path.split(FILE_SEP)[-1] in ALLOWED_FILE_SUFFIX: return True else: return False def generate_summary_content(content: list, file_list, prefix=&quot;&quot;): for f in file_list.values(): content.append(LINK_TEMPLATE.format(prefix=prefix, name=f[&quot;name&quot;], path=f[&quot;path&quot;])) if f[&quot;child&quot;]: content = generate_summary_content(content, f[&quot;child&quot;], prefix=&quot;{} &quot;.format(prefix)) return content def write_summary_file(content: str, file_path): with open(file_path, &quot;w&quot;) as f: f.write(content) if __name__ == '__main__': import json wk_path = &quot;./&quot; src_path = os.path.join(wk_path) print(&quot;src path: &quot;, os.path.abspath(src_path)) file_list = get_md_files(src_path) print(&quot;file list: &quot;, json.dumps(file_list)) content = [SUMMARY_TEMPLATE] content = generate_summary_content(content, file_list) content_str = &quot;\\n&quot;.join(content) print(&quot;content: &quot;, content_str) file_path = os.path.join(src_path, SUMMARY_TEMPLATE_FILENAME) print(&quot;will write file: &quot;, file_path) write_summary_file(content_str, file_path) print(&quot;write summary complete&quot;) ","link":"https://blog.wenmingmin.site/post/personaltoolsdeploy/"},{"title":"MariaDB视图","content":"本质上视图是个虚拟表,可以查询,更新,会将数据更新到基础表. CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW [IF NOT EXISTS] view_name [(column_list)] AS select_statement [WITH [CASCADED | LOCAL] CHECK OPTION] ALTER [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW view_name [(column_list)] AS select_statement [WITH [CASCADED | LOCAL] CHECK OPTION] 注意点: 创建view时,sql中不能包含子查询; 使用Replace时,view存在则等同于alter; 默认为with cascaded check option,即在更新视图时,需要满足视图的约束条件.可以使用with local check option跳过检查; 视图中的字段是固定化的,即create view name as select * from table后,view的字段和table是一致的,但alter table新增字段后,视图并不包含table新增的字段; view定义的select语句中不能出现oreder by,除非使用Top(limit),此时order by只是为了满足top数量要求; 视图默认使用undefined算法,即由数据库自己决定使用merge还是temptable算法,更倾向于merge,默认为merge; merge算法:合并使用视图的sql语句以及视图定义的语句,替换为基础表的表名和字段名; temptable算法: 将视图定义的语句执行的结果放入临时表,然后使用临时表进行操作; 包含这些情况不使用merge算法: having, limit, group by, distinct, union, union all, select中有子查询, 没有基础表, 使用了聚合函数(MAX, COUNT, SUM等); 使用check table view_name来检查无效视图, 因为视图创建后,基础表可能删除或者更新视图用到的字段; 嵌套视图可能会出现列错位等导致列不对等的情况,导致读取数据错误,因为创建嵌套视图后更新了base视图; 参考 https://ignorantshr.github.io/person-blog/mysql/15.%20MySQL%E8%A7%86%E5%9B%BE/#11-mysql ","link":"https://blog.wenmingmin.site/post/mariadb-shi-tu/"},{"title":"Docker的代理","content":"Docker的代理 国内相关的docker代理均被ban了，所以需要自己搭建代理，可以借助于serverless功能， 白嫖cloudflare这个互联网大善人。 推荐给本地的dockerd守护进程配置使用网络代理。 不建议使用cloudflare来代理Docker Hub。 cloudflare已经更新了条款，不再允许借助CDN来作为代理服务使用，有封号的风险。 cloudflare已经更新了条款，不再允许借助CDN来作为代理服务使用，有封号的风险。 cloudflare已经更新了条款，不再允许借助CDN来作为代理服务使用，有封号的风险。 dockerd配置代理 sudo mkdir -p /etc/systemd/system/docker.service.d sudo touch /etc/systemd/system/docker.service.d/proxy.conf 然后编辑文件/etc/systemd/system/docker.service.d/proxy.conf，写入以下内容（其中的代理地址请使用自己的网络代理配置），保存更改后重启Docker。 [Service] Environment=&quot;HTTP_PROXY=http://127.0.0.1:7890/&quot; Environment=&quot;HTTPS_PROXY=http://127.0.0.1:7890/&quot; Environment=&quot;NO_PROXY=localhost,127.0.0.1,.example.com&quot; Docker Hub 代理 参考 https://github.com/cmliu/CF-Workers-docker.io ： 先注册一个cloudflare 账户，使用免费的订阅计划（Free plan）。 进入cloudflare dashboard -&gt; 账户主页 -&gt; 开发人员平台 -&gt; 创建应用程序 -&gt; pages 。 这个时候二选一： 如果fork了上面的github repo, 那就使用 连接到git ，一步一步按照流程， 部署fork的repo; 或者 复制 _work.js 的代码， 直接部署; 等待部署完成后， 会有一个cloudflare域(名)，使用该域名（也可以设置pages的自定义域，使用自己个人的域名来指向这个代理）可以直接访问，显示的是docker hub的搜索框， 此时代理步骤完成。 docker pull 使用代理 最简单的方式是在pull的镜像名称前加上域名：docker pull 代理域名/镜像名称:镜像标签 还可以配置docker的repo mirror, 编辑/etc/docker/daemon.json文件： { &quot;registry-mirrors&quot;: [&quot;https://代理域名/&quot;] } 然后重启docker ， 执行docker info查看registries信息，是否为代理的域名。 docker build使用代理 在build命令中增加参数--build-arg http_proxy=http://代理地址 --build-arg https_proxy=http://代理地址 或者修改文件~/.docker/config.json， 此修改对docker run、docker build生效： { &quot;proxies&quot;: { &quot;default&quot;: { &quot;httpProxy&quot;: &quot;http://代理地址&quot;, &quot;httpsProxy&quot;: &quot;http://代理地址&quot;, &quot;noProxy&quot;: &quot;localhost,127.0.0.1&quot; } } } 修改后，如果不想在容器中使用代理， 将容器中环境变量（http_proxy、https_proxy）设置为空即可。 需要注意的是代理地址需要根据容器使用的网络模式（host、bridge）来变动。 ","link":"https://blog.wenmingmin.site/post/docker-de-dai-li/"},{"title":"MySQL超时参数","content":"MySQL数据库timeout有关参数 timeout相关参数 使用 set {global | session} {arg_name}={value} 命令来设置这些参数 MariaDB [(none)]&gt; show variables like '%timeout%'; +----------------------------+----------+ | Variable_name | Value | +----------------------------+----------+ | connect_timeout | 10 | | deadlock_timeout_long | 50000000 | | deadlock_timeout_short | 10000 | | delayed_insert_timeout | 300 | | innodb_lock_wait_timeout | 50 | | innodb_rollback_on_timeout | OFF | | interactive_timeout | 100 | | lock_wait_timeout | 31536000 | | net_read_timeout | 30 | | net_write_timeout | 60 | | slave_net_timeout | 60 | | thread_pool_idle_timeout | 60 | | wait_timeout | 100 | +----------------------------+----------+ 13 rows in set (0.00 sec) MySQL test@172.20.16.9:(none)&gt; show variables like '%timeout%'; +-----------------------------+----------+ | Variable_name | Value | +-----------------------------+----------+ | connect_timeout | 10 | | delayed_insert_timeout | 300 | | have_statement_timeout | YES | | innodb_flush_log_at_timeout | 1 | | innodb_lock_wait_timeout | 50 | | innodb_rollback_on_timeout | OFF | | interactive_timeout | 28800 | | lock_wait_timeout | 31536000 | | net_read_timeout | 30 | | net_write_timeout | 60 | | rpl_stop_slave_timeout | 31536000 | | slave_net_timeout | 60 | | wait_timeout | 28800 | +-----------------------------+----------+ 13 rows in set 参数说明 connect_timeout 在获取连接阶段(authenticate)起作用，获取MySQL连接是多次握手的结果，除了用户名和密码的匹配校验外，还有IP-&gt;HOST-&gt;DNS-&gt;IP验证，任何一步都可能因为网络问题导致线程阻塞。为了防止线程浪费在不必要的校验等待上，超过connect_timeout的连接请求将会被拒绝。 delayed_insert_timeout 为MyISAM INSERT DELAY设计的超时参数，表示INSERT DELAY handler线程在INSERT DELAY语句终止前等待这个INSERT语句的时间，注意是表示insert delay延迟插入的超时时间，不是insert语句。默认值是300S，从5.6.7开始被弃用（因为delayed insert功能被弃用）后续版本将移除。 innodb_lock_wait_timeout(innodb的dml操作的行级锁的等待时间) innodb使用这个参数能够有效避免在资源有限的情况下产生太多的锁等待；指的是事务等待获取资源时等待的最长时间，超过这个时间还未分配到资源则会返回应用失败；参数的时间单位是秒，最小可设置为1s(一般不会设置得这么小)，最大可设置1073741824秒(34年)，默认安装时这个值是50s，超过这个时间会报 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction。 innodb_rollback_on_timeout 默认情况下innodb_lock_wait_timeout 超时后只是超时的sql执行失败，整个事务并不回滚，也不做提交，如需要事务在超时的时候回滚，则需要设置innodb_rollback_on_timeout=ON，该参数默认为OFF。 interactive_timeout &amp; wait_timeout 在连接空闲阶段(sleep)起作用，即使没有网络问题，也不能允许客户端一直占用连接。对于保持sleep状态超过了wait_timeout(或interactive_timeout，取决于client_interactive标志)的客户端，MySQL会主动断开连接。 lock_wait_timeout 获取MDL锁的等待时间，默认值是31536000秒=1年。 凡是需要获取MDL锁的操作都受到这个参数的影响，不单单是DDL语句，包含在表上的DML、DDL操作，以及视图、存储过程、存储函数、lock table，flush table with read lock语句等。但不适用于隐式访问系统表的语句，如：grant和revoke等。 net_read_timeout &amp; net_write_timeout 在连接繁忙阶段(query)起作用，即使连接没有处于sleep状态，即客户端忙于计算或者存储数据，MySQL也选择了有条件的等待。在数据包的分发过程中，客户端可能来不及响应（发送、接收、或者处理数据包太慢）。为了保证连接不被浪费在无尽的等待中，MySQL也会选择有条件（net_read_timeout和net_write_timeout）地主动断开连接。 这个参数只对TCP/IP链接有效，只针对在Activity状态下的线程有效。 slave_net_timeout Slave判断主库是否挂掉的超时设置，在设定时间内依然没有获取到Master的回应就认为Master已经挂掉了，后续根据超时重连参数设置进行重连主库的操作。默认值：3600s。 innodb_flush_log_at_timeout 5.6.6引入，参数innodb_flush_log_at_trx_commit=1时，此超时参数不起作用，当innodb_flush_log_at_trx_commit=0/2时才起作用。5.6.6之后表示每innodb_flush_log_at_timeout秒一次的频率刷新redo log(在5.6.6之前是固定每秒一次刷新redo log，5.6.6之后刷新频率可以通过这个参数设置，当然，这个参数本身默认值也是1S)。 rpl_semi_sync_master_timeout 为semi-sync复制时，主库在某次事务提交时，如果等待超过rpl_semi_sync_master_timeout多秒之后仍然没有接收到任何从库做回包响应，那么主库自动降级为异步复制模式，当主库探测到有备库恢复回包时，主库自动恢复到semi-sync复制模式。默认值为10000毫秒=10秒。 rpl_stop_slave_timeout 5.6.13之后引入的参数，控制stop slave 的执行时间，在重放一个大的事务的时候,突然执行stop slave,命令 stop slave会执行很久,这个时候可能产生死锁或阻塞,严重影响性能，可以通过rpl_stop_slave_timeout参数控制stop slave 的执行时间。默认值是31536000秒=1年。 ","link":"https://blog.wenmingmin.site/post/mysql-chao-shi-can-shu/"},{"title":"MySQL行长度限制(MySQL Row Length Limit)","content":" 行大小限制 COMPACT行格式 数据类型 CHAR VARCHAR NULL标识字段 长度表示位 TEXT InnoDB索引长度限制 VARCHAR和TEXT MySQL对于每个表都有一些硬限制，例如不超过4096列，但是实际上有效值是多少，取决于几个因素： 表的最大行大小限制了列数量，所有的列的总长度不能超过行的最大值； 列的存储字段类型限制，例如存储格式、字符集等； 存储引擎的限制，例如InnoDB要求每个表不能超过1017列； 功能部分实现为被隐藏的列； 行大小限制 由以下因素限制： 表内部限制最大为65535字节，存储引擎最大支持也是这个值，BLOB、TEXT类型的值超长时会将内容存储到其他page，让page可以存储更多的行； 对于4KB，8KB，16KB和32KB的innodb_page_size设置，表的最大行大小(本地存储)略小于page的一半。例如默认的16KB page，最大行大小略小于8KB，64KB page的最大行大小限制为16KB； 可变长度字段类型，当可变类型字段值得长度超过行大小，则将超出的内容存储到外部page； 不同存储格式使用了不同数量的page元数据，这也会影响行大小； COMPACT行格式 COMPACT行格式按顺序包含： 固定长度为5字节的记录头信息：描述本行记录的相关信息，如是否删除、下一个记录的位置，是否叶子节点等； 可变字段长度列表：用于存储变长字段所占的字节数，其中超过255字节的字段使用2字节表示，未超过255则使用1字节表示； 空值列表； 字段值列表存储列本身的数据外，还存储6字节长度的事务id以及7字节长度的roll point，未设置主键的表数据还会存储6字节长度的行id。超出768字节的字符串，超出的部分会存储在溢出页中，并生成20字节的溢出页地址用于指向溢出页； DYNAMIC行格式不存储可变长度字段的数据，只存储数据所在地址； 数据类型 CHAR 最长255字符，真实长度据取决于字符集； VARCHAR 最大长度 = （最大行大小(默认为65535字节) - NULL标识列占用字节数 - 长度标识字节数）/ 字符集单字符最大字节数。 NULL标识字段 如果有一个列允许为空，则需要1 bit来标识，每8 bits的标识会组成一个字段，该字段会存放在每行最开始的位置。如果有N个NULL 字段，则标识符占用的长度就是N/8 向上取整。 根据存储的实现： 可以考虑用varchar替代tinytext 如果需要非空的默认值，就必须使用varchar 如果存储的数据大于64K，就必须使用到mediumtext , longtext varchar(255+)和text在存储机制是一样的 需要特别注意varchar(255)不只是255byte ,实质上有可能占用的更多。 特别注意，varchar大字段一样的会降低性能，所以在设计中还是一个原则大字段要拆出去，主表还是要尽量的瘦小. 长度表示位 存储开销是小于255只要1字节、大于255后使用两字节。是因为按照可能的数据大小，分为0 - 255(28)、256 - 65535(216)，刚好对应1字节和2字节。 但要注意，其计算根据的是字段声明的字符长度、计算可能的字节数(取编码的最大长度，如utf8 是3字节，而utf8mb4 是4字节)，再决定长度标志的字节数。如VARCHAR(100)，字符集为UTF8，可能的字节数为300，长度标识则为2字节。 MySQL中声明的类型长度都是字符数，而计算长度时需要转换成字节。 TEXT TEXT最大长度为65535（2^16 − 1）个字符。如果是多字节字符，则有效最大长度会更少。存储时会增加2字节的前缀、标识长度。 TEXT无法使用临时表。因为TEXT列可能很大，临时表空间会膨胀的非常快，所以MYSQL的MEMORY引擎不支持这类大的数据类型。 如果临时表列包括了TEXT类型，MySQL会直接用磁盘上的表、而不是内存中的表。 磁盘比内存的I/O效率低很多，这就意味着性能急剧降低。 建议避免使用select *，它会选择所有列，而是在已经确定结果集范围后、左联获取对应的TEXT字段；可以将TEXT单独拆出一个表，这样读写时减少与该列发生关系的可能，性能也会提升。 InnoDB索引长度限制 索引最大长度为767字节，索引会限制单独Key的最大长度为 767 字节，超过这个长度必须建立小于等于 767 字节的前缀索引。如果是utf-8 就是 255个字符，这也恰恰是能建索引情况下的最大值。如果是utf8mb4 , 则只能767 / 4 = 191 个字符。 修改索引限制长度需要在my.ini配置文件中添加以下内容(innodb_large_prefix=1，修改单列索引字节长度为767的限制，单列索引的长度变为307 )，并重启。但是开启该参数后还需要开启表的动态存储或压缩： 系统变量innodb_file_format为Barracuda，并且ROW_FORMAT为DYNAMIC或COMPRESSED。 VARCHAR和TEXT VARCHAR是标准类型，但是TEXT不是。TEXT需要进行overflow存储，即768字节数据存在行中，其余的数据存储在其他page中。这种做法是为了让一个page可以容纳更多的行。 两者的区别还有默认值上的区别(TEXT不允许有默认值)，以及字段额外开销的区别： # overhead是指需要几个字节用于记录该字段的实际长度 varchar 小于255byte 1byte overhead varchar 大于255byte 2byte overhead tinytext 0-255 1 byte overhead text 0-65535 byte 2 byte overhead mediumtext 0-16M 3 byte overhead longtext 0-4Gb 4byte overhead TEXT如果不超过大约8KB时，超出768字节的部分不会存在别的page上。 ","link":"https://blog.wenmingmin.site/post/mysql-xing-chang-du-xian-zhi-mysql-row-length-limit/"},{"title":"golang指针方法和值方法","content":"type MyInterFace interface { GetName() string GetId() uint } type MyStruct struct { Name string Id uint } func (m *MyStruct) GetName() string { return m.Name } func (m MyStruct) GetId() uint { return m.Id } 无论是值类型还是指针类型,都能调用interface声明的方法. 但是区别: 当值类型时,发生值拷贝,上述方法访问是不同的地址; 指针类型则指向的是同一内存地址; ","link":"https://blog.wenmingmin.site/post/golang-zhi-zhen-fang-fa-he-zhi-fang-fa/"}]}